{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b1cac6-e646-44b8-8a27-d4e5757b3333",
   "metadata": {},
   "source": [
    "## Testing Category Discovery Using KCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e614314-6ccf-4ead-913a-5df891908447",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from kcm.koopman_category_model import KoopmanCategoryModel\n",
    "from kcm.discovery import sup_con_loss, BaselineModel, HASHHead, cluster_acc, split_cluster_acc_v1, split_cluster_acc_v2, create_hash_ids\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ce3bb-b36c-4efe-a1b4-aeded7d249fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cats = 10\n",
    "num_samples = 500\n",
    "system_dimension = 1\n",
    "delay_embeddings = 3\n",
    "num_segments = 10\n",
    "svd_rank = None\n",
    "dmd_rank = None\n",
    "q = 1\n",
    "num_clusters = 7\n",
    "test_size = 0.2\n",
    "codebook_training_size = 30 # divides <num training classes> * num_segments \n",
    "category_discovery=True\n",
    "train_classes = range(7)\n",
    "seed = 42\n",
    "\n",
    "KCM = KoopmanCategoryModel(num_cats=num_cats,\n",
    "                           num_samples=num_samples,\n",
    "                           system_dimension=system_dimension,\n",
    "                           delay_embeddings=delay_embeddings,\n",
    "                           num_segments=num_segments,\n",
    "                           svd_rank=svd_rank,\n",
    "                           dmd_rank=dmd_rank,\n",
    "                           q=q,\n",
    "                           cluster_method='kmeans',\n",
    "                           num_clusters=num_clusters,\n",
    "                           noisy_data=False,\n",
    "                           normalize_inputs=False,\n",
    "                           seed=seed)\n",
    "\n",
    "KCM.generate_data()\n",
    "KCM.train_test_split(test_size=test_size,\n",
    "                     codebook_training_size=codebook_training_size,\n",
    "                     category_discovery=category_discovery,\n",
    "                     train_classes=train_classes)\n",
    "KCM.create_codebook(include_plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b109a-152a-4095-989b-dec76df46fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "KCM.save()\n",
    "KCM.shutdown_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011dcfb-7bdd-46bc-a026-12e905f972e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KCM = KoopmanCategoryModel.load(r\"runs\\KCM_20250723_105850_7514df2b\\model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977602ef-83c4-417a-b798-3a294afd68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = KCM.inv_c_train_matrix\n",
    "y_train = KCM.train_target\n",
    "\n",
    "X_test = KCM.inv_c_test_matrix\n",
    "y_test = KCM.test_target\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.tensor(y_train).long().squeeze()\n",
    "\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.tensor(y_test).long().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865072d-b2e3-4198-ba41-1cbcd1d3f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique() # 80, 0, 80, 80 = 240 training samples (5 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42803d14-c609-48f7-a97c-4e366de74403",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.unique() # 20, 100, 20, 20 = 160 testing samples (5 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35924f2-abcc-46bb-b04d-66eef2a69c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_histograms, indices = torch.unique(X_train, dim=0, return_inverse=True)\n",
    "\n",
    "print('Histograms with Duplicate Labels:\\n')\n",
    "num_issue_hists = 0\n",
    "num_issue_samples = 0\n",
    "for uni, ind in zip(unique_histograms,torch.unique(indices)):\n",
    "\n",
    "    histogram_count = X_train[indices == ind].shape[0]\n",
    "    unique_labels = torch.unique(y_train[indices == ind])\n",
    "\n",
    "    if len(unique_labels) > 1:\n",
    "        num_issue_hists += 1\n",
    "        num_issue_samples += histogram_count\n",
    "        print(f'{np.round(np.array(uni),3)} has {histogram_count} instances with {np.array(unique_labels)} unique_labels')\n",
    "\n",
    "\n",
    "percent_hist_issues = round(num_issue_hists*100/len(unique_histograms),2)\n",
    "print(f'\\n{num_issue_hists}/{len(unique_histograms)} ({percent_hist_issues} %) of histograms had duplicate labels,')\n",
    "perc_sample_issues = round(num_issue_samples*100/(KCM.num_cats*KCM.num_samples),2)\n",
    "print(f'\\taffecting {num_issue_samples}/{KCM.num_cats*KCM.num_samples} samples ({perc_sample_issues} %)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e23396-b406-45a2-9526-ae2d2dc7a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = 8\n",
    "hidden_dims = [200, 200] # [1024, 512, 256]\n",
    "dropout = 0.3\n",
    "classes = KCM.num_cats\n",
    "test_size = 0.25\n",
    "epochs = 1000\n",
    "model_type = 'SMILE' # baseline, SMILE\n",
    "\n",
    "if model_type == 'baseline':\n",
    "    model = BaselineModel(input_dim, output_dim, hidden_dims, dropout)\n",
    "elif model_type == 'SMILE':\n",
    "    model = HASHHead(input_dim, output_dim, hidden_dims, dropout)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "true_y = KCM.df_test[['target','sample']].drop_duplicates()['target'].values\n",
    "mask = np.isin(true_y,KCM.train_classes)\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_aris = []\n",
    "train_nmis = []\n",
    "train_amis = []\n",
    "\n",
    "test_aris = []\n",
    "test_nmis = []\n",
    "test_amis = []\n",
    "\n",
    "weighted_scores = []\n",
    "old_scores = []\n",
    "new_scores = []\n",
    "\n",
    "global_scores = []\n",
    "old_global_scores = []\n",
    "new_global_scores = []\n",
    "\n",
    "unique_training_hashes = []\n",
    "unique_testing_hashes = []\n",
    "\n",
    "losses = []\n",
    "for i in tqdm(range(epochs), desc='Training Model'):\n",
    "\n",
    "    if model_type == 'baseline':\n",
    "        training_features, _, _ = model(X_train)\n",
    "        testing_features, _, _ = model(X_test)\n",
    "        sc_loss = sup_con_loss(training_features, y_train, temp=0.2)\n",
    "        reg_loss = 0\n",
    "        \n",
    "    elif model_type == 'SMILE':\n",
    "        training_features, training_hash_features, _ = model(X_train)\n",
    "        testing_features, testing_hash_features, _ = model(X_test)\n",
    "        sc_loss = sup_con_loss(training_features, y_train, temp=0.2)\n",
    "\n",
    "        reg_loss = (1 - torch.abs(training_hash_features)).mean()\n",
    "        \n",
    "\n",
    "    loss = sc_loss * 1 + reg_loss * 3\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Create Cluster ids for train, test, and full data sets\n",
    "    training_hash_ids, training_hashes, new_training_labels = create_hash_ids(training_features, y_train)\n",
    "    testing_hash_ids, testing_hashes, new_testing_labels = create_hash_ids(testing_features, y_test)\n",
    "    \n",
    "    # # Full dataset clusters\n",
    "    # features, _, _ = model(X)\n",
    "    # full_has_ids, full_hashes, new_full_labels = create_hash_ids(features, y)\n",
    "    \n",
    "    # Report ARI, NMI, and AMI for training and testing splits\n",
    "    train_aris.append(adjusted_rand_score(y_train, training_hash_ids))\n",
    "    train_nmis.append(normalized_mutual_info_score(y_train, training_hash_ids))\n",
    "    train_amis.append(adjusted_mutual_info_score(y_train, training_hash_ids))\n",
    "\n",
    "    test_aris.append(adjusted_rand_score(y_test, testing_hash_ids))\n",
    "    test_nmis.append(normalized_mutual_info_score(y_test, testing_hash_ids))\n",
    "    test_amis.append(adjusted_mutual_info_score(y_test, testing_hash_ids))\n",
    "\n",
    "    # Report category discovery metrics\n",
    "    total_acc, old_acc, new_acc = split_cluster_acc_v1(np.array(y_test), testing_hash_ids, mask)\n",
    "    weighted_scores.append(total_acc)\n",
    "    old_scores.append(old_acc)\n",
    "    new_scores.append(new_acc)\n",
    "    \n",
    "    total_acc, old_acc, new_acc = split_cluster_acc_v2(np.array(y_test), testing_hash_ids, mask)\n",
    "    global_scores.append(total_acc)\n",
    "    old_global_scores.append(old_acc)\n",
    "    new_global_scores.append(new_acc)\n",
    "\n",
    "    unique_training_hashes.append(len(np.unique(training_hash_ids)))\n",
    "    unique_testing_hashes.append(len(np.unique(testing_hash_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d870-5986-4967-9720-da83ea9192eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(np.log(np.array(losses)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Supervised Contrastive Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363941b9-932f-4e2c-9754-6be25ff16fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(unique_training_hashes,label='Training')\n",
    "plt.plot(unique_testing_hashes,label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Unique Hashes')\n",
    "plt.title('Number of Unique Hashes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac0775-fe8f-4847-9c87-8c1dc28510fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs, axes = plt.subplots(1,3,figsize=(12,3))\n",
    "\n",
    "minimum = -0.05\n",
    "maximum = 1.05\n",
    "\n",
    "# Plot independent & weighted scores\n",
    "axes[0].plot(old_scores,linewidth=0.8,color='blue',label='Old Classes')\n",
    "axes[0].plot(new_scores,linewidth=0.8,color='red',label='New Classes')\n",
    "axes[0].plot(weighted_scores,linewidth=2,color='purple',label='Total')\n",
    "axes[0].set_title('Independent Scores')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim([minimum, maximum])\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot global scores\n",
    "axes[1].plot(old_global_scores,linewidth=0.8,color='blue',label='Old Classes')\n",
    "axes[1].plot(new_global_scores,linewidth=0.8,color='red',label='New Classes')\n",
    "axes[1].plot(global_scores,linewidth=2,color='purple',label='Total')\n",
    "axes[1].set_title('Global Scores')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylim([minimum, maximum])\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot training and testing aris\n",
    "axes[2].plot(train_aris,linewidth=2,color='lightblue',label='Train ARI')\n",
    "axes[2].plot(train_nmis,linewidth=1,color='mediumblue',label='Train NMI')\n",
    "axes[2].plot(train_amis,linewidth=0.5,color='darkblue',label='Train AMI')\n",
    "axes[2].plot(test_aris,linewidth=2,color='lightcoral',label='Test ARI')\n",
    "axes[2].plot(test_nmis,linewidth=1,color='red',label='Test NMI')\n",
    "axes[2].plot(test_amis,linewidth=0.5,color='darkred',label='Test AMI')\n",
    "axes[2].set_title('')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylim([minimum, maximum])\n",
    "axes[2].legend()\n",
    "\n",
    "\n",
    "plt.suptitle('Category Discovery Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe47363-f018-4f00-8772-9fd859946c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics (means, variances, etc.) - to construct a feature vector for comparison\n",
    "# Basic Statistics:\n",
    "    # derivatives (do weighted averaging before taking derivatives, try out a couple different time-scales)\n",
    "    # average distance between zero crossings\n",
    "    # amplitude variance\n",
    "    # average change in amplitude\n",
    "    # angular momentums\n",
    "    # periodicity\n",
    "    # average distance traveled over time (is the system straying from zero over time?)\n",
    "    # \n",
    "\n",
    "# Keep the number features on the same magnitude for both feature extraction methods\n",
    "# I can calculate a lot of basic statistcs, but maybe filter down which ones I use based on importance\n",
    "    # Consider using shap to filter down which features to use from the basic statistics\n",
    "\n",
    "# Try introducing noise to see how koopman feature extractor compares with other methods - statistical methods will suffer big time with noise\n",
    "# Comparing with random features for comparison to random\n",
    "\n",
    "# Play around with the size of the encoder (more parameters than features?)\n",
    "# Try making num_parameters < num_data inputs (maybe ~20%) - make only 2 layers with smaller hidden dimensions\n",
    "\n",
    "# Change size of hash vector\n",
    "\n",
    "# How to augment the dataset for optimized category discovery - add more categories\n",
    "# Can training on more classes help discover more categories?\n",
    "# Test out changing training and testing class sizes\n",
    "# Compare train on 1-3 and discover 3-1\n",
    "\n",
    "# test out different signal to noise ratios as a study\n",
    "\n",
    "\n",
    "# Observations\n",
    "\n",
    "# With a smaller neural network, it just bunches a lot of the samples into the same categories, particulary the non-harmonic ones\n",
    "# The network has higher accuracy at the very beginning - this could be due to the repetitive nature of the histograms from KCM\n",
    "# There are some histogram values that are repeated across different systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6ad91-ab3b-4718-bcde-6059db8df862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(df,combine=True,y_to_color=None,all_y_vals=None):\n",
    "\n",
    "    all_clusters = sorted(df['cluster_id'].unique())\n",
    "    unique_y = sorted(df['y'].unique()) if all_y_vals is None else sorted(all_y_vals)\n",
    "    \n",
    "    x = np.arange(len(all_clusters))\n",
    "    width = 0.8 / len(unique_y)\n",
    "\n",
    "    if y_to_color is None:\n",
    "        cmap = plt.get_cmap('tab10')\n",
    "        y_to_color = {y: cmap(i % 10) for i, y in enumerate(unique_y)}\n",
    "        \n",
    "\n",
    "    if combine:\n",
    "        plt.figure(figsize=(8,3))\n",
    "        for i, y_val in enumerate(unique_y):\n",
    "            counts = df.loc[df['y'].eq(y_val),'cluster_id'].value_counts().reindex(all_clusters, fill_value=0)\n",
    "            pmf = counts / counts.sum()\n",
    "            bar_positions = x + i * width\n",
    "            plt.bar(bar_positions, pmf, width=width, label=f'y = {y_val}', color=y_to_color.get(y_val, 'gray'))\n",
    "        \n",
    "        plt.xlabel('cluster_id')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Cluster ID Value Counts by y')\n",
    "        plt.xticks(x + width * (len(unique_y) - 1) / 2, labels=all_clusters)\n",
    "        plt.legend(title='y value')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    else:\n",
    "        fig, axs = plt.subplots(len(unique_y), 1, figsize=(6, 10), constrained_layout=True)\n",
    "        for i, y_val in enumerate(unique_y):\n",
    "            counts = train.loc[train['y'].eq(y_val),'cluster_id'].value_counts().reindex(all_clusters, fill_value=0)\n",
    "            pmf = counts / counts.sum()\n",
    "            axs[i].bar(all_clusters, pmf, color=y_to_color.get(y_val, 'gray'))\n",
    "            axs[i].set_title(f'Histogram of cluster_ids for y = {y_val}')\n",
    "            axs[i].set_xlabel('cluster_id')\n",
    "            axs[i].set_ylabel('Frequency')\n",
    "            axs[i].set_xticks(all_clusters)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737efec-b441-4e9e-8b7b-a3a7f12cf1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'cluster_id' : training_cluster_ids, 'y' : y_train}\n",
    "data = {'cluster_id' : training_hash_ids, 'y' : y_train}\n",
    "train = pd.DataFrame(data)\n",
    "\n",
    "# data = {'cluster_id' : testing_cluster_ids, 'y' : y_test}\n",
    "data = {'cluster_id' : testing_hash_ids, 'y' : y_test}\n",
    "test = pd.DataFrame(data)\n",
    "\n",
    "for i in range(output_dim):\n",
    "    train[f'hash_{i}'] = training_hashes[:,i]\n",
    "    test[f'hash_{i}'] = testing_hashes[:,i]\n",
    "\n",
    "all_y_vals = sorted(set(train['y']).union(set(test['y'])))\n",
    "\n",
    "# Create consistent color mapping\n",
    "cmap = plt.get_cmap('tab10')\n",
    "y_to_color = {y: cmap(i % 10) for i, y in enumerate(all_y_vals)}\n",
    "\n",
    "\n",
    "plot_histograms(train,combine=True,y_to_color=y_to_color)\n",
    "plot_histograms(test,combine=True,y_to_color=y_to_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7d499-3e70-47f3-a325-d4127f0d7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0,1,0,1,0,1,0,1]\n",
    "# [1,1,1,1,0,0,0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
