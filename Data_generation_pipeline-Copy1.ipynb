{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71685bd-f5fa-47f6-8024-0462acc20afa",
   "metadata": {},
   "source": [
    "## Clustering with DMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6075d-5366-4d27-8d40-658052d0dd86",
   "metadata": {},
   "source": [
    "[Based off of Koopman Operator Framework for Time Series Modeling and Analysis](https://link.springer.com/article/10.1007/s00332-017-9441-y) by Amit Surana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb684d0-1748-4ccc-b579-082fb89292c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydmd import DMD\n",
    "from pydmd.plotter import plot_summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.optimize import linprog\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import inspect\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6ba9e6-4f01-413a-b0d4-37228d8fad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- reproducibility header ---------------------------------------------\n",
    "SEED = 42                          # pick any integer and keep it fixed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)   # fixes hash-based shuffling\n",
    "random.seed(SEED)                           # builtin random\n",
    "np.random.seed(SEED)                        # NumPy legacy RNG\n",
    "\n",
    "# Preferred modern RNG you can pass around\n",
    "rng = np.random.default_rng(SEED)\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb3d0cc-25e1-4644-b5c8-b9e15c7544ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoopmanCategoryModel:\n",
    "    def __init__(self, num_cats=None, num_samples=None, data_path=None, delay_embeddings=0, num_segments=5,\n",
    "                 svd_rank=None, dmd_rank=None, q=1, cluster_method='kmeans', num_clusters=None, run_root=\"runs\"):\n",
    "\n",
    "        logger = logging.getLogger(\"KCM\")\n",
    "        \n",
    "        # ---------- unique output directory ----------\n",
    "        ts   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        uid  = uuid.uuid4().hex[:8]                 # 8-char suffix\n",
    "        self.run_dir = Path(run_root) / f\"KCM_{ts}_{uid}\"\n",
    "        self.run_dir.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        \n",
    "        # ----------------- robust logger ------------------\n",
    "        self.logger = logging.getLogger(f\"KCM.{uid}\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.propagate = False\n",
    "\n",
    "        # # console\n",
    "        # ch = logging.StreamHandler()\n",
    "        # ch.setFormatter(logging.Formatter(\"%(levelname)s | %(message)s\"))\n",
    "        # self.logger.addHandler(ch)\n",
    "\n",
    "        # file\n",
    "        fh = logging.FileHandler(self.run_dir / \"run.log\")\n",
    "        fh.setFormatter(logging.Formatter(\n",
    "            \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "            \"%Y-%m-%d %H:%M:%S\"))\n",
    "        self.logger.addHandler(fh)\n",
    "\n",
    "        self.logger.info(\"Results will be saved in %s\", self.run_dir)\n",
    "\n",
    "        \n",
    "        # Dataset Parameters\n",
    "        self.num_cats = num_cats\n",
    "        self.num_samples = num_samples\n",
    "        self.num_segments = num_segments\n",
    "        self.data_path = data_path\n",
    "        self.dataset = self._load_data(data_path)\n",
    "        self.cats = list(self.dataset.keys())\n",
    "        self.m, self.n = self.dataset[self.cats[0]][0]['y'].shape\n",
    "        self.segment_length = int(self.n/self.num_segments)\n",
    "\n",
    "        # DMD Parameters\n",
    "        self.delay_embeddings = delay_embeddings\n",
    "        self.total_observables = self.m * (self.delay_embeddings + 1)\n",
    "        self.svd_rank = self.total_observables if svd_rank is None else svd_rank\n",
    "        self.dmd_rank = self.svd_rank if dmd_rank is None else dmd_rank\n",
    "\n",
    "        if (self.dmd_rank > self.svd_rank) or (self.svd_rank > self.total_observables) or (self.dmd_rank > self.total_observables):\n",
    "            raise ValueError(f'Should have dmd_rank < svd_rank < total_observables, but have values of {self.dmd_rank}, {self.svd_rank}, and {self.total_observables}')\n",
    "\n",
    "        if np.mod(self.n,self.num_segments) != 0:\n",
    "            raise ValueError(f'Number of segments {self.num_segments} must divide given data size {self.n-self.delay_embeddings}')\n",
    "        \n",
    "        # Clustering Parameters\n",
    "        self.q = q\n",
    "        self.MDS_dimension = 10\n",
    "        self.cluster_method = cluster_method\n",
    "        self.num_clusters = num_clusters\n",
    "        self.labels = None\n",
    "        \n",
    "        # Classification Parameters\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # Category Discovery Parameters\n",
    "\n",
    "\n",
    "        # Save off Parameter Details\n",
    "        self.num_cats = num_cats\n",
    "        self.num_samples = num_samples\n",
    "        self.num_segments = num_segments\n",
    "        self.data_path = data_path\n",
    "        self.dataset = self._load_data(data_path)\n",
    "        self.cats = list(self.dataset.keys())\n",
    "        self.m, self.n = self.dataset[self.cats[0]][0]['y'].shape\n",
    "        self.segment_length = int(self.n/self.num_segments)\n",
    "\n",
    "        # DMD Parameters\n",
    "        self.delay_embeddings = delay_embeddings\n",
    "        self.total_observables = self.m * (self.delay_embeddings + 1)\n",
    "        self.svd_rank = self.total_observables if svd_rank is None else svd_rank\n",
    "        self.dmd_rank = self.svd_rank if dmd_rank is None else dmd_rank\n",
    "\n",
    "\n",
    "        param_details = [f'num_cats: {self.num_cats}',\n",
    "                         f'num_samples: {self.num_samples}',\n",
    "                         f'num_segments: {self.num_segments}',\n",
    "                         f'data_path: {self.data_path}',\n",
    "                         f'cats (categories) : {self.cats}',\n",
    "                         f'delay_embeddings: {self.delay_embeddings}',\n",
    "                         f'total_observables: {self.total_observables}',\n",
    "                         f'svd_rank: {self.svd_rank}',\n",
    "                         f'dmd_rank: {self.dmd_rank}',\n",
    "                         f'q: {self.q}',\n",
    "                         f'MDS_dimension: {self.MDS_dimension}',\n",
    "                         f'cluster_method: {self.cluster_method}',\n",
    "                         f'num_cluseters (k) : {self.num_clusters}']\n",
    "\n",
    "        joined = '\\n'.join(param_details)\n",
    "        self.logger.info(f\"Parameters:\\n{joined}\")\n",
    "\n",
    "\n",
    "    def _load_data(self, data_path):\n",
    "        \"\"\"\n",
    "        Load in the data based on either the provided data path\n",
    "        or the default path if none is provided.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.data_path is None:\n",
    "            path = rf\"data\\1-dimensional-systems\\dataset_{self.num_cats}_class_{self.num_samples}_samples.pkl\"\n",
    "            with open(path, 'rb') as f:\n",
    "                dataset = pickle.load(f)\n",
    "        else:\n",
    "            with open(self.data_path, 'rb') as f:\n",
    "                dataset = pickle.load(f)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "\n",
    "        all_eigs = []\n",
    "        all_modes = []\n",
    "        all_amps = []\n",
    "        all_data = []\n",
    "        \n",
    "        self.total_dmd_calculations = self.num_cats * self.num_samples * self.num_segments\n",
    "        self.logger.info(f'Generating {self.total_dmd_calculations} DMD eigs/modes each with dimensionality {self.svd_rank}')\n",
    "        \n",
    "        for cat in self.cats:\n",
    "        \n",
    "            curr_data = self.dataset[cat]\n",
    "        \n",
    "            for index in range(self.num_samples):\n",
    "        \n",
    "                X = curr_data[index]['y'].T\n",
    "                \n",
    "                if self.delay_embeddings > 0:\n",
    "                    X = np.hstack([X[i:self.n-self.delay_embeddings+i,:] for i in range(self.delay_embeddings+1)])\n",
    "                \n",
    "                # DMD\n",
    "                for sp in range(self.num_segments):\n",
    "                    start_ind = sp * self.segment_length\n",
    "                    end_ind = (sp + 1) * self.segment_length\n",
    "                    X_split = X[start_ind:end_ind,:]\n",
    "                    \n",
    "                    eigs, modes, b = self._compute_dmd(X_split.T)\n",
    "                    all_eigs.append(eigs)\n",
    "                    all_modes.append(modes)\n",
    "                    all_amps.append(b)\n",
    "                    all_data.append(X_split)\n",
    "\n",
    "        self.all_eigs = all_eigs\n",
    "        self.all_modes = all_modes\n",
    "        self.all_amps = all_amps\n",
    "        self.all_data = all_data\n",
    "        \n",
    "        if len(self.all_eigs) != self.total_dmd_calculations:\n",
    "            raise ValueError(f'Incorrect Number of DMD Run: expected {self.total_dmd_calculations}, but ran {len(self.all_eigs)}')\n",
    "\n",
    "        self.num_observables = min([self.svd_rank, self.total_observables])\n",
    "        if self.all_eigs[0].shape[0] != self.num_observables:\n",
    "            raise ValueError(f'Incorrect Observables Count: expected {self.num_observables}, but got {self.all_eigs[0].shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "        # Combine DMD Data into Dataframe\n",
    "        real_eigs = []\n",
    "        imag_eigs = []\n",
    "        full_modes = []\n",
    "        \n",
    "        for i in range(len(self.all_eigs)):\n",
    "            eigs = self.all_eigs[i]\n",
    "            modes = self.all_modes[i]\n",
    "            \n",
    "            real_eigs.append(eigs.real)\n",
    "            imag_eigs.append(eigs.imag)\n",
    "        \n",
    "            norm_modes = [np.linalg.norm(modes[:,k]) / sum(np.linalg.norm(modes,axis=0)) for k in range(self.num_observables)]\n",
    "            \n",
    "            full_modes.append(norm_modes)\n",
    "        \n",
    "        real_eigs = np.array(real_eigs)\n",
    "        imag_eigs = np.array(imag_eigs)\n",
    "        full_modes = np.array(full_modes)\n",
    "        target = np.array([i for i in range(self.num_cats) for _ in range(self.num_segments) for _ in range(self.num_samples)])[:,np.newaxis]\n",
    "        sample = np.array([j for _ in range(self.num_cats) for j in range(self.num_samples) for _ in range(self.num_segments)])[:,np.newaxis]\n",
    "        segment = np.array([k for _ in range(self.num_cats) for _ in range(self.num_samples) for k in range(self.num_segments)])[:,np.newaxis]\n",
    "        \n",
    "        columns = [f'eig_{i}' for i in range(self.num_observables)] + [f'real_{i}' for i in range(self.num_observables)] + [f'imag_{i}' for i in range(self.num_observables)] + [f'norm_mode_{i}' for i in range(self.num_observables)] + ['target','sample','segment']\n",
    "        df = pd.DataFrame(data=np.hstack([np.array(self.all_eigs),real_eigs,imag_eigs,full_modes,target,sample,segment]),columns=columns)\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if 'eig' not in col:\n",
    "                df[col] = df[col].astype(float)\n",
    "        df['target'] = df['target'].astype(int)\n",
    "        df['sample'] = df['sample'].astype(int)\n",
    "        df['segment'] = df['segment'].astype(int)\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def train_test_split(self, test_size, codebook_training_size):\n",
    "\n",
    "        self.test_size = test_size\n",
    "        n_splits = int(1 / self.test_size)\n",
    "        \n",
    "        sgkf  = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        groups = self.df['sample']\n",
    "        y      = self.df['target']\n",
    "        \n",
    "        train_idx, test_idx = next(sgkf.split(self.df, y=y, groups=groups))\n",
    "        self.df_train = self.df.iloc[train_idx].reset_index(drop=True)\n",
    "        self.df_test  = self.df.iloc[test_idx].reset_index(drop=True)\n",
    "        \n",
    "        self.logger.info(f'Training set size: {self.df_train.shape[0]}')\n",
    "        self.logger.info(f'Testing set size: {self.df_test.shape[0]}')\n",
    "        \n",
    "        \n",
    "        self.codebook_training_size = codebook_training_size\n",
    "        size = self.codebook_training_size // self.num_cats // self.num_segments\n",
    "        \n",
    "        samples_to_keep = self.df_train[['target','sample']].drop_duplicates().groupby('target').apply(lambda g: g.sample(size, random_state=SEED)).reset_index(drop=True)\n",
    "        self.df_sample = self.df.merge(samples_to_keep, on=[\"target\", \"sample\"], how=\"inner\")\n",
    "        \n",
    "        self.logger.info(f'Codebook training size: {self.codebook_training_size}')\n",
    "\n",
    "\n",
    "\n",
    "    def create_codebook(self,include_plots=False):\n",
    "\n",
    "        # Create Wasserstein Distance Matrix from downsampled training points\n",
    "        metric_matrix = self._create_metric_matrix(self.df_sample)\n",
    "\n",
    "        \n",
    "        # # Optimize embedding dimension for metric matrix reconstruction accuracy\n",
    "        # self.MDS_dimension = self._optimize_MDS_dim() # Need to write internal function\n",
    "        \n",
    "\n",
    "        # Create Euclidean Representation Based off Distance Matrix\n",
    "        embedding = MDS(n_components=self.MDS_dimension, dissimilarity='precomputed')\n",
    "        self.X_transformed = embedding.fit_transform(metric_matrix)\n",
    "        \n",
    "        D_reconstructed = squareform(pdist(self.X_transformed))\n",
    "\n",
    "        if include_plots:\n",
    "            plt.imshow(D_reconstructed)\n",
    "            plt.colorbar()\n",
    "            plt.title('Reconstructed Wasserstein Distance Matrix') # Include reconstruction percent error\n",
    "            self._save_current_fig(\"distance_matrix\")\n",
    "\n",
    "\n",
    "        if self.cluster_method == 'kmeans':\n",
    "            self.centroid, label, inertia = k_means(self.X_transformed, self.num_clusters, random_state=SEED)\n",
    "        \n",
    "        # Find the 5 closest points in the training data to the cluster centers (surrogate cluster centers)\n",
    "        distances = cdist(self.centroid,self.X_transformed)\n",
    "        cluster_indices = np.argmin(distances, axis=1)\n",
    "        self.cluster_centers = self.X_transformed[cluster_indices,:]\n",
    "        \n",
    "        # Use those surrogate cluster centers to make the codebook dataframe\n",
    "        self.codebook = self.df_sample.loc[cluster_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Reassign the labels for the downsampled dataframe based on surrogate cluster centers\n",
    "        distances = cdist(self.cluster_centers,self.X_transformed)\n",
    "        self.sample_label = np.argmin(distances,axis=0)\n",
    "        \n",
    "        # Compare percent change in labels from using surrogate cluster centers\n",
    "        tol = 1e-8\n",
    "        num_changed = np.sum(abs(self.sample_label - label) >= tol)\n",
    "        self.logger.info(f'{num_changed}/{self.df_sample.shape[0]} labels changed ({round(num_changed/self.df_sample.shape[0] * 100,2)}%) when choosing training points as cluster centers')\n",
    "\n",
    "\n",
    "        # Represent training and testing datasets with codebook\n",
    "        self.train_metric_matrix = self._create_metric_matrix(self.codebook,self.df_train)\n",
    "        self.test_metric_matrix = self._create_metric_matrix(self.codebook,self.df_test)\n",
    "\n",
    "        # Assign training samples to clusters\n",
    "        train_label = np.argmin(self.train_metric_matrix,axis=0)\n",
    "        self.df_train['cluster'] = train_label\n",
    "        \n",
    "        \n",
    "        all_clusters = sorted(self.df_train['cluster'].unique())\n",
    "        \n",
    "        if include_plots:\n",
    "\n",
    "            plt.imshow(self.train_metric_matrix[:,:100])\n",
    "            plt.colorbar()\n",
    "            plt.title('Training Metric Matrix (100 pts)')\n",
    "            self._save_current_fig(\"train_metric_matrix\")\n",
    "\n",
    "            plt.imshow(self.test_metric_matrix[:,:100])\n",
    "            plt.colorbar()\n",
    "            plt.title('Testing Metric Matrix (100 pts)')\n",
    "            self._save_current_fig(\"test_metric_matrix\")\n",
    "            \n",
    "            for c in self.df_train.target.unique():\n",
    "                counts = self.df_train.loc[self.df_train.target.eq(c),'cluster'].value_counts().reindex(all_clusters, fill_value=0).sort_index()\n",
    "                counts.plot(kind='bar',figsize=(3,2), title=f'{self.cats[c]}')\n",
    "                plt.xlabel('Cluster')\n",
    "                plt.ylabel('Count')\n",
    "                plt.tight_layout()\n",
    "                self._save_current_fig(f\"{c}_clusters\")\n",
    "\n",
    "            # Plot the data\n",
    "            self.plot_data(3)\n",
    "            self.plot_MDS(color_by_target=True)\n",
    "            self.plot_MDS(color_by_target=False)\n",
    "\n",
    "\n",
    "        # Identify number of samples from each system category in both training and testing samples\n",
    "        self.num_train_samples = int(len(self.df_train.groupby(['target','sample']).groups) / self.num_cats)\n",
    "        self.num_test_samples = int(len(self.df_test.groupby(['target','sample']).groups) / self.num_cats)\n",
    "        \n",
    "        # Create cluster assignments for each segment in training and testing set\n",
    "        self.train_cluster_assignments = np.argmin(self.train_metric_matrix,axis=0).reshape(self.num_cats,self.num_train_samples,self.num_segments)\n",
    "        self.test_cluster_assignments = np.argmin(self.test_metric_matrix,axis=0).reshape(self.num_cats,self.num_test_samples,self.num_segments)\n",
    "        \n",
    "        # Count number of each cluster assignment in training set\n",
    "        self.N_ks = np.array([(np.sum(self.train_cluster_assignments == i,axis=2) > 0).ravel().sum() for i in range(self.num_clusters)])\n",
    "        self.N = self.num_cats * self.num_samples # Total number of time series samples\n",
    "        \n",
    "        # Create histogram of cluster assignments for training data\n",
    "        self.n_train_matrix = np.concatenate([np.sum(self.train_cluster_assignments == i,axis=2).ravel() for i in range(self.num_clusters)]).reshape(self.num_clusters,self.num_cats*self.num_train_samples).T\n",
    "        self.c_train_matrix = self.n_train_matrix / self.num_segments\n",
    "        self.inv_c_train_matrix = self.c_train_matrix * np.log(self.N / self.N_ks)\n",
    "        \n",
    "        # Create histogram of cluster assignments for testing data\n",
    "        self.n_test_matrix = np.concatenate([np.sum(self.test_cluster_assignments == i,axis=2).ravel() for i in range(self.num_clusters)]).reshape(self.num_clusters,self.num_cats*self.num_test_samples).T\n",
    "        self.c_test_matrix = self.n_test_matrix / self.num_segments\n",
    "        self.inv_c_test_matrix = self.c_test_matrix * np.log(self.N / self.N_ks)\n",
    "        \n",
    "        # Extract target assignments (classification labels)\n",
    "        self.train_target = self.df_train['target'].values[-self.num_segments::-self.num_segments][-1::-1]\n",
    "        self.test_target = self.df_test['target'].values[-self.num_segments::-self.num_segments][-1::-1]\n",
    "\n",
    "        # Save Model\n",
    "        self.save()\n",
    "        \n",
    "\n",
    "    def perform_classification(self, classifier=None, return_statement=False):\n",
    "        \"\"\"\n",
    "        Perform classification with given classifer\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        class_name = classifier.__class__.__name__\n",
    "        self.logger.info(f'Classifier Chosen: {class_name}')\n",
    "        \n",
    "        \n",
    "        # Prepare classifier\n",
    "        self.y_pred = {}\n",
    "        \n",
    "        for c_train, c_test, name in [(self.c_train_matrix, self.c_test_matrix, 'reg_c'),(self.inv_c_train_matrix, self.inv_c_test_matrix, 'inverse_c')]:\n",
    "\n",
    "            # Fit and score classifier\n",
    "            classifier.fit(c_train, self.train_target)\n",
    "            y_pred = classifier.predict(c_test)\n",
    "            self.y_pred[name] = y_pred\n",
    "            \n",
    "            score = classifier.score(c_test,self.test_target)\n",
    "            self.logger.info(f'{name} score: {score}')\n",
    "\n",
    "            # Generate confusion matrix\n",
    "            cm = confusion_matrix(self.test_target, y_pred)\n",
    "            self.logger.info(\"Confusion Matrix:\\n%s\", cm)\n",
    "\n",
    "            \n",
    "            # Save confusion matrix figure\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                        xticklabels=[f'Predicted {cat}' for cat in self.cats],\n",
    "                        yticklabels=[f'True {cat}' for cat in self.cats])\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.title(f'Confusion Matrix {score}:\\n{class_name}, {name}\\n')\n",
    "            self._save_current_fig(f\"confusion_matrix_{class_name}_{name}\")\n",
    "        \n",
    "        if return_statement:\n",
    "            return cm, y_pred\n",
    "\n",
    "\n",
    "    def save(self, name: str = \"model.pkl\"):\n",
    "        \"\"\"Save *this* model object to <run_dir>/<name>.\"\"\"\n",
    "        path = self.run_dir / name\n",
    "        joblib.dump(self, path)\n",
    "        self.logger.info(\"Model saved to %s\", path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"Load a previously-saved model.\"\"\"\n",
    "        return joblib.load(path)\n",
    "\n",
    "        \n",
    "\n",
    "    def _compute_dmd(self, X_full):\n",
    "        \"\"\"\n",
    "        Compute DMD Eigenvalues, Modes, and Amplitudes using numpy matrix operations\n",
    "        \"\"\"\n",
    "\n",
    "        m, n = X_full.shape\n",
    "    \n",
    "        if self.svd_rank < 0 or self.svd_rank > m:\n",
    "            raise ValueError(f'Given svd rank is {self.svd_rank} yet data given only has dimension {m}')\n",
    "        \n",
    "        X = X_full[:,:-1]\n",
    "        Y = X_full[:,1:]\n",
    "        \n",
    "        # SVD of X\n",
    "        U, s, Vh = np.linalg.svd(X, full_matrices=False)\n",
    "        \n",
    "        # r rank of svd of X\n",
    "        U_r = U[:,:self.svd_rank]\n",
    "        s_r = s[:self.svd_rank]\n",
    "        Vh_r = Vh[:self.svd_rank,:]\n",
    "        \n",
    "        # Calculate A_tilde\n",
    "        U_r_star = U_r.conj().T\n",
    "        V_r = Vh_r.conj().T\n",
    "        S_r_inv = np.diag(1.0 / s_r)\n",
    "        \n",
    "        A_tilde = U_r_star @ Y @ V_r @ S_r_inv\n",
    "        \n",
    "        # Eigendecomposition of A_tilde\n",
    "        eigs, W = np.linalg.eig(A_tilde)\n",
    "        \n",
    "        # Koopman Modes & amplitudes\n",
    "        modes = Y @ V_r @ S_r_inv @ W\n",
    "        b, _, _, _ = np.linalg.lstsq(modes,X[:,0])\n",
    "    \n",
    "        return eigs, modes, b\n",
    "\n",
    "\n",
    "    def _create_metric_matrix(self, df1, df2=None, plot_matrix=False):\n",
    "\n",
    "        square_matrix = False\n",
    "\n",
    "        # df2 should only be 'None' when creating codebook from self.df_sample \n",
    "        if df2 is None:\n",
    "            square_matrix = True\n",
    "            df2 = df1.copy()\n",
    "            dim1 = dim2 = df1.shape[0]\n",
    "            total_metric_calculations = int(self.codebook_training_size**2/2)\n",
    "\n",
    "            # Square Matrix\n",
    "            metric_statement = f'(1/2) * {self.codebook_training_size}^2 = {total_metric_calculations} Wasserstein distance metrics'\n",
    "\n",
    "        else:\n",
    "            dim1, dim2 = df1.shape[0], df2.shape[0]\n",
    "            total_metric_calculations = int(dim1 * dim2)\n",
    "            metric_statement = f'{dim1} * {dim2} = {total_metric_calculations} Wasserstein distance metrics'\n",
    "        \n",
    "        metric_matrix = np.zeros((dim1,dim2))\n",
    "        \n",
    "        \n",
    "        # Columns for extracting data\n",
    "        eig_columns = [col for col in df1.columns if 'eig' in col]\n",
    "        norm_mode_columns = [col for col in df1.columns if 'norm_mode' in col]\n",
    "\n",
    "        \n",
    "        df1.reset_index(drop=True,inplace=True)\n",
    "        df2.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        for i in tqdm(df1.index, desc=metric_statement):\n",
    "            \n",
    "            # Only include half of computations if square matrix\n",
    "            df2_indices = range(i) if square_matrix else df2.index\n",
    "            \n",
    "            for j in df2_indices:\n",
    "        \n",
    "                # Eigenvalues\n",
    "                l1 = df1.loc[i,eig_columns].values[:,np.newaxis]\n",
    "                l2 = df2.loc[j,eig_columns].values[:,np.newaxis]\n",
    "                \n",
    "                # Mass vectors (normed modes)\n",
    "                m1 = df1.loc[i,norm_mode_columns].values.real\n",
    "                m2 = df2.loc[j,norm_mode_columns].values.real\n",
    "        \n",
    "                # Compute Wasserstein Metric\n",
    "                metric_matrix[i,j] = self._compute_wasserstein_metric(l1,l2,m1,m2)\n",
    "\n",
    "        if square_matrix:\n",
    "            i_triu = np.triu_indices_from(metric_matrix, k=1)\n",
    "            metric_matrix[i_triu] = metric_matrix.T[i_triu]\n",
    "\n",
    "        return metric_matrix\n",
    "\n",
    "\n",
    "        \n",
    "    def _compute_wasserstein_metric(self, l1, l2, m1, m2):\n",
    "        \n",
    "        # --- Step 1: Sample Inputs ---\n",
    "        n = self.num_observables\n",
    "        n_bar = self.num_observables\n",
    "    \n",
    "        m1 = m1 / np.sum(m1)\n",
    "        m2 = m2 / np.sum(m2)\n",
    "        \n",
    "        assert np.isclose(np.sum(m1), 1.0)\n",
    "        assert np.isclose(np.sum(m2), 1.0)\n",
    "        \n",
    "        # --- Step 2: Compute cost matrix C (shape n x n_bar) ---\n",
    "        C = np.linalg.norm(l1[:, None, :] - l2[None, :, :], axis=2) ** 2\n",
    "        # Flatten to 1D for linprog\n",
    "        c = C.flatten()  # size (n * n_bar,)\n",
    "        \n",
    "        # --- Step 3: Equality Constraints ---\n",
    "        \n",
    "        # Total number of variables\n",
    "        N = n * n_bar\n",
    "        \n",
    "        # 1. Row sum constraints: each row i must sum to m[i]\n",
    "        A_eq_rows = np.zeros((n, N))\n",
    "        for i in range(n):\n",
    "            for j in range(n_bar):\n",
    "                A_eq_rows[i, i * n_bar + j] = 1\n",
    "        b_eq_rows = m1\n",
    "        \n",
    "        # 2. Column sum constraints: each column j must sum to m_bar[j]\n",
    "        A_eq_cols = np.zeros((n_bar, N))\n",
    "        for j in range(n_bar):\n",
    "            for i in range(n):\n",
    "                A_eq_cols[j, i * n_bar + j] = 1\n",
    "        b_eq_cols = m2\n",
    "        \n",
    "        # 3. Total mass constraint\n",
    "        A_eq_total = np.ones((1, N))\n",
    "        b_eq_total = np.array([1.0])\n",
    "        \n",
    "        # Combine constraints\n",
    "        A_eq = np.vstack([A_eq_rows, A_eq_cols, A_eq_total])\n",
    "        b_eq = np.concatenate([b_eq_rows, b_eq_cols, b_eq_total])\n",
    "        \n",
    "        # --- Step 4: Bounds ---\n",
    "        bounds = [(0, None)] * N  # rho_ij >= 0\n",
    "        \n",
    "        # --- Step 5: Solve ---\n",
    "        result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
    "        \n",
    "        # --- Step 6: Extract solution ---\n",
    "        if result.success:\n",
    "            rho_star = result.x.reshape((n, n_bar))\n",
    "            transport_cost = np.sum(rho_star * C)\n",
    "            wasserstein_metric = transport_cost ** (1 / self.q)\n",
    "        else:\n",
    "            self.logger.info(\"Optimization failed:\", result.message)\n",
    "    \n",
    "        return wasserstein_metric\n",
    "\n",
    "\n",
    "    def shutdown_logger(self):\n",
    "        \"\"\"Immediately close all handlers to release the log file.\"\"\"\n",
    "        for h in self.logger.handlers[:]:\n",
    "            h.close()\n",
    "            self.logger.removeHandler(h)\n",
    "\n",
    "    def reconstruction_error(self):\n",
    "        pass\n",
    "\n",
    "    def _optimize_MDS_dim(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Main pipeline to fit DMD and optionally a classifier.\n",
    "        X: np.ndarray of shape (n_samples, n_timesteps, n_features)\n",
    "        y: Optional labels for classification\n",
    "        \"\"\"\n",
    "        features = self._extract_dmd_features(X)\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "\n",
    "        if y is not None:\n",
    "            self.classifier.fit(features_scaled, y)\n",
    "        else:\n",
    "            self._perform_clustering(features_scaled)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict categories (either clusters or classes)\n",
    "        \"\"\"\n",
    "        features = self._extract_dmd_features(X)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "\n",
    "        if hasattr(self.classifier, \"predict\"):\n",
    "            return self.classifier.predict(features_scaled)\n",
    "        else:\n",
    "            return self._perform_clustering(features_scaled, return_labels=True)\n",
    "\n",
    "    def _extract_dmd_features(self, X):\n",
    "        \"\"\"\n",
    "        Apply DMD to each sample and return vectorized Koopman features.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for sample in X:\n",
    "            modes = self._compute_dmd(sample)\n",
    "            features.append(modes.flatten().real)  # Simplified; you can include eigenvalues, etc.\n",
    "        return np.array(features)\n",
    "\n",
    "    def _perform_clustering(self, features, return_labels=False):\n",
    "        if self.cluster_method == 'kmeans':\n",
    "            self.kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "            self.kmeans.fit(features)\n",
    "            if return_labels:\n",
    "                return self.kmeans.predict(features)\n",
    "            self.labels = self.kmeans.labels_\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\n",
    "            'dmd_rank': self.dmd_rank,\n",
    "            'cluster_method': self.cluster_method,\n",
    "            'classifier': self.classifier.__class__.__name__\n",
    "        }\n",
    "\n",
    "    def _out(self, name: str, ext=\"png\"):\n",
    "        \"\"\"Return full path inside this run’s directory.\"\"\"\n",
    "        return self.run_dir / f\"{name}.{ext}\"\n",
    "\n",
    "\n",
    "    def _save_current_fig(self, name: str, ext=\"png\", dpi=300):\n",
    "        plt.gcf().savefig(self.run_dir / f\"{name}.{ext}\", dpi=dpi,\n",
    "                          bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_data(self, samples_to_plot=3):\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=self.num_cats,\n",
    "            # specs=[[{'type': 'scene'}]*num_cats],\n",
    "            # subplot_titles=[f'System {i+1}' for i in range(num_cats)]\n",
    "            subplot_titles=[val.replace('_','<br>') for val in self.cats]\n",
    "        )\n",
    "\n",
    "        samples_to_plot = self.num_samples if samples_to_plot is None else samples_to_plot\n",
    "        \n",
    "        for i, cat in enumerate(self.cats):\n",
    "        \n",
    "            for j in range(samples_to_plot):\n",
    "                trace = go.Scatter(\n",
    "                    x=self.dataset[cat][j]['t'], y=self.dataset[cat][j]['y'][1],\n",
    "                    mode='lines',\n",
    "                    line=dict(width=1),\n",
    "                    name=f\"{cat}\"\n",
    "                )\n",
    "            \n",
    "                # Add trace to the correct subplot\n",
    "                fig.add_trace(trace, row=1, col=i+1)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            # title=dict(x=0.5, text=\"3D Line Plot Subplots\"),\n",
    "            height=350,\n",
    "            width=1300,\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig.update_yaxes(range=[-20, 20])\n",
    "\n",
    "        fig.write_html(self.run_dir / \"time_series_samples.html\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_MDS(self,color_by_target=True):\n",
    "\n",
    "\n",
    "        if color_by_target:\n",
    "            title = f'Data Clusters (3/{self.MDS_dimension} dimensions)<br>Colored by True Target'\n",
    "            data_color = self.df_sample.target\n",
    "            true_centroid_color = 'black'\n",
    "            data_centroid_color = 'green'\n",
    "        else:\n",
    "            title = f'Data Clusters (3/{self.MDS_dimension} dimensions)<br>Colored by Cluster Label'\n",
    "            data_color = self.sample_label\n",
    "            true_centroid_color = np.arange(1,self.num_clusters+1)\n",
    "            data_centroid_color = 'black'\n",
    "    \n",
    "            \n",
    "        full_data = go.Scatter3d(x=self.X_transformed[:,0],\n",
    "                                 y=self.X_transformed[:,1],\n",
    "                                 z=self.X_transformed[:,2],\n",
    "                                 mode='markers',\n",
    "                                 marker=dict(color=data_color,size=3),\n",
    "                                 name='Data')\n",
    "    \n",
    "        centers = go.Scatter3d(x=self.centroid[:,0],\n",
    "                               y=self.centroid[:,1],\n",
    "                               z=self.centroid[:,2],\n",
    "                               mode='markers',\n",
    "                               marker=dict(color=true_centroid_color,size=8),\n",
    "                               name='Centroids')\n",
    "        \n",
    "        data_centers = go.Scatter3d(x=self.cluster_centers[:,0],\n",
    "                                    y=self.cluster_centers[:,1],\n",
    "                                    z=self.cluster_centers[:,2],\n",
    "                                    mode='markers',\n",
    "                                    marker=dict(color=data_centroid_color,size=8),\n",
    "                                    name='Surrogate Centroids')\n",
    "    \n",
    "\n",
    "        data = [full_data,centers,data_centers]\n",
    "\n",
    "        \n",
    "        layout=go.Layout(width=900,height=600,\n",
    "                         title=dict(x=0.5,text=title))\n",
    "        \n",
    "        fig = go.Figure(data=data,layout=layout)\n",
    "\n",
    "        name = 'colored_by_target' if color_by_target else 'colored_by_cluster'\n",
    "        fig.write_html(self.run_dir / f\"MDS_{name}.html\")\n",
    "        \n",
    "\n",
    "    def create_phase_diagrams(self, num_to_plot=20, inv_c_pred=True):\n",
    "\n",
    "        # Choose y_predictions based on whether inv_c matrix was used or not\n",
    "        c_name = 'inverse_c' if inv_c_pred else 'reg_c'\n",
    "        y_pred = self.y_pred[c_name]\n",
    "        \n",
    "        # Extract Dynamic System Names\n",
    "        train_sets = self.df_train[['target', 'sample']].drop_duplicates().reset_index(drop=True)\n",
    "        test_sets = self.df_test[['target', 'sample']].drop_duplicates().reset_index(drop=True)\n",
    "        systems = [cat.replace('_',' ').capitalize() for cat in self.cats]\n",
    "\n",
    "\n",
    "        # Identify max and min position and velocity for plotting\n",
    "        max_x = 0\n",
    "        max_v = 0\n",
    "        for i in range(len(self.all_data)):\n",
    "            x = max(abs(self.all_data[i][:,0]))\n",
    "            v = max(abs(self.all_data[i][:,1]))\n",
    "            if x > max_x:\n",
    "                max_x = x\n",
    "            if v > max_v:\n",
    "                max_v = v\n",
    "\n",
    "\n",
    "        # Plot Phase Diagrams for Full Test Set\n",
    "        test_index = 0\n",
    "        \n",
    "        for s, system in tqdm(enumerate(systems), desc=f'Creating Phase Diagrams'):\n",
    "            \n",
    "            figs, axes = plt.subplots(5,4,figsize=(14,14)) # Should be based on num_to_plot\n",
    "            ax = axes.ravel()\n",
    "            \n",
    "            for ind in range(num_to_plot):\n",
    "                \n",
    "                # Identify target & sample number\n",
    "                target = test_sets.loc[test_index,'target']\n",
    "                sample = test_sets.loc[test_index,'sample']\n",
    "                \n",
    "                # Identify segment indices in self.all_data\n",
    "                start = (self.num_samples * self.num_segments) * target + self.num_segments * sample\n",
    "                segment_indices = np.arange(start,start+5)\n",
    "                \n",
    "                start = self.num_samples\n",
    "                for i,d in enumerate(segment_indices):\n",
    "                \n",
    "                    x, dx = self.all_data[d][:,:2].T\n",
    "                    \n",
    "                    ax[ind].axhline(y=0, color='k', linewidth=2)\n",
    "                    ax[ind].axvline(x=0, color='k', linewidth=2)\n",
    "                    ax[ind].plot(x,dx,label=f'Segment {i+1}')\n",
    "                    for j in range(0, len(x)-1, 50):\n",
    "                        ax[ind].annotate(\"\",\n",
    "                                     xy=(x[j+1], dx[j+1]),\n",
    "                                     xytext=(x[j], dx[j]),\n",
    "                                     arrowprops=dict(arrowstyle=\"->\", color=\"blue\", lw=1))\n",
    "                \n",
    "                title = [f'True: {systems[target]}',\n",
    "                         f'Pred: {systems[y_pred[test_index]]}']\n",
    "            \n",
    "                if self.test_target[test_index] == y_pred[test_index]:\n",
    "                    ax[ind].set_facecolor('palegreen')\n",
    "                else:\n",
    "                    ax[ind].set_facecolor('lightcoral')\n",
    "                \n",
    "                ax[ind].set_title('\\n'.join(title))\n",
    "                ax[ind].set_xlim([-np.ceil(abs(max_x)),np.ceil(abs(max_x))])\n",
    "                ax[ind].set_ylim([-np.ceil(max_v),np.ceil(max_v)])\n",
    "                ax[ind].grid()\n",
    "                ax[ind].legend(fontsize=6)\n",
    "        \n",
    "                test_index += 1\n",
    "        \n",
    "            nrows, ncols = 5, 4\n",
    "        \n",
    "            for i, a in enumerate(ax):\n",
    "                row = i // ncols\n",
    "                col = i % ncols\n",
    "            \n",
    "                if row == nrows - 1:  # bottom row\n",
    "                    a.set_xlabel(\"x\")\n",
    "            \n",
    "                if col == 0:  # leftmost column\n",
    "                    a.set_ylabel(\"dx/dt\")\n",
    "        \n",
    "            \n",
    "            plt.suptitle(f'{system} Phase Diagrams\\n({c_name} training matrix)',fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            self._save_current_fig(f\"results_{self.cats[s]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ecd9e6-bf29-4df7-9c0d-4906aa76d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/2) * 50^2 = 1250 Wasserstein distance metrics: 100%|████████████████████████████████| 50/50 [00:03<00:00, 13.33it/s]\n",
      "5 * 2000 = 10000 Wasserstein distance metrics: 100%|█████████████████████████████████████| 5/5 [00:28<00:00,  5.73s/it]\n",
      "5 * 500 = 2500 Wasserstein distance metrics: 100%|███████████████████████████████████████| 5/5 [00:07<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "num_cats = 5\n",
    "num_samples = 100\n",
    "data_path = rf\"data\\1-dimensional-systems\\dataset_{num_cats}_class_{num_samples}_samples.pkl\"\n",
    "delay_embeddings = 1 # 3\n",
    "num_segments = 5\n",
    "svd_rank = None\n",
    "dmd_rank = None\n",
    "q = 2\n",
    "num_clusters = 5\n",
    "test_size = 0.2\n",
    "codebook_training_size = 50\n",
    "\n",
    "KCM = KoopmanCategoryModel(num_cats=num_cats,\n",
    "                           num_samples=num_samples,\n",
    "                           data_path=data_path,\n",
    "                           delay_embeddings=delay_embeddings,\n",
    "                           num_segments=num_segments,\n",
    "                           svd_rank=svd_rank,\n",
    "                           dmd_rank=dmd_rank,\n",
    "                           q=q,\n",
    "                           cluster_method='kmeans',\n",
    "                           num_clusters=num_clusters)\n",
    "\n",
    "KCM.generate_data()\n",
    "KCM.train_test_split(test_size=test_size,\n",
    "                     codebook_training_size=codebook_training_size)\n",
    "KCM.create_codebook(include_plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc31633f-500b-44d6-ae6c-a076486536c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Phase Diagrams: 5it [01:09, 13.99s/it]\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "KCM.perform_classification(classifier=classifier,\n",
    "                           return_statement=False)\n",
    "KCM.create_phase_diagrams(inv_c_pred=True)\n",
    "KCM.shutdown_logger()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
